{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_csv(\n",
    "    \"../data/csv/playlists_dataset/spotify_dataset.csv\", on_bad_lines=\"skip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = base_df[[\"trackname\", \"playlistname\", \"artistname\", \"user_id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the var types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trackname       string[python]\n",
      "playlistname    string[python]\n",
      "artistname      string[python]\n",
      "user_id         string[python]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "clean_df[\"trackname\"] = clean_df[\"trackname\"].astype(\"string\")\n",
    "clean_df[\"playlistname\"] = clean_df[\"playlistname\"].astype(\"string\")\n",
    "clean_df[\"artistname\"] = clean_df[\"artistname\"].astype(\"string\")\n",
    "clean_df[\"user_id\"] = clean_df[\"user_id\"].astype(\"string\")\n",
    "\n",
    "print(clean_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining trackname and artistname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the trackname and artistname into one string identify individual songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"track_and_artist\"] = clean_df[\"trackname\"] + \" \" + clean_df[\"artistname\"]\n",
    "clean_df[\"playlist_and_user_id\"] = clean_df[\"playlistname\"] + \" \" + clean_df[\"user_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering wrong playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty sure that the extremly long playlists exist because of equal names of different playlists like \"love\" or \"rock\" ... \n",
    "Need to investigate if every user id is connected to one playlist or how it works out if multiple users work on one playlist... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_sizes = clean_df.groupby(\"playlist_and_user_id\").size()\n",
    "\n",
    "# Compute the lower and upper quantiles\n",
    "lower_n_percent_threshold = playlist_sizes.quantile(0.20)\n",
    "upper_n_percent_threshold = playlist_sizes.quantile(0.98)\n",
    "\n",
    "# Keep only those playlists within the specified size range\n",
    "valid_playlists = playlist_sizes[\n",
    "    (playlist_sizes >= lower_n_percent_threshold) & \n",
    "    (playlist_sizes <= upper_n_percent_threshold)\n",
    "].index\n",
    "\n",
    "# Filter the DataFrame to keep only valid playlists\n",
    "filtered_df = clean_df[clean_df[\"playlist_and_user_id\"].isin(valid_playlists)]\n",
    "\n",
    "clean_df = filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest playlist length: 10\n"
     ]
    }
   ],
   "source": [
    "# Recompute the group sizes in the filtered DataFrame\n",
    "final_playlist_sizes = clean_df.groupby(\"playlist_and_user_id\").size()\n",
    "\n",
    "# Find the smallest playlist length\n",
    "smallest_playlist_length = final_playlist_sizes.min()\n",
    "\n",
    "print(\"Smallest playlist length:\", smallest_playlist_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"../data/csv/playlists_dataset/playlist_data_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/csv/playlists_dataset/playlist_data_v3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_playlist_np = clean_df.groupby(\"playlist_and_user_id\")[\n",
    "    \"track_and_artist\"\n",
    "].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_playlist = tokenized_playlist_np.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "temp_arr = tokenized_playlist_np\n",
    "\n",
    "extrapolated_data = tokenized_playlist\n",
    "\n",
    "# Shuffle each sub-array n times and collect results\n",
    "for subarray in temp_arr:\n",
    "    for _ in range(n):\n",
    "        extrapolated_data.append(np.random.permutation(subarray).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735012"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extrapolated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = extrapolated_data\n",
    "\n",
    "# split value\n",
    "split_fac = 0.9\n",
    "\n",
    "max_idx = len(tokenized_data) - 1\n",
    "anchor = int(max_idx * split_fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tokenized_playlist[:anchor]\n",
    "test_set = tokenized_playlist[anchor:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tokenized_data/playlist_names/dataset_train_v3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(training_set, f)\n",
    "\n",
    "\n",
    "with open(\"../data/tokenized_data/playlist_names/dataset_test_v3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_set, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
