{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_csv(\n",
    "    \"../data/csv/playlists_dataset/spotify_dataset.csv\", on_bad_lines=\"skip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = base_df[[\"trackname\", \"playlistname\", \"artistname\", \"user_id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the var types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trackname       string[python]\n",
      "playlistname    string[python]\n",
      "artistname      string[python]\n",
      "user_id         string[python]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "clean_df[\"trackname\"] = clean_df[\"trackname\"].astype(\"string\")\n",
    "clean_df[\"playlistname\"] = clean_df[\"playlistname\"].astype(\"string\")\n",
    "clean_df[\"artistname\"] = clean_df[\"artistname\"].astype(\"string\")\n",
    "clean_df[\"user_id\"] = clean_df[\"user_id\"].astype(\"string\")\n",
    "\n",
    "print(clean_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining trackname and artistname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the trackname and artistname into one string identify individual songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"track_and_artist\"] = clean_df[\"trackname\"] + \" \" + clean_df[\"artistname\"]\n",
    "clean_df[\"playlist_and_user_id\"] = clean_df[\"playlistname\"] + \" \" + clean_df[\"user_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering wrong playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty sure that the extremly long playlists exist because of equal names of different playlists like \"love\" or \"rock\" ... \n",
    "Need to investigate if every user id is connected to one playlist or how it works out if multiple users work on one playlist... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_sizes = clean_df.groupby(\"playlist_and_user_id\").size()\n",
    "\n",
    "# Compute the lower and upper quantiles\n",
    "lower_n_percent_threshold = playlist_sizes.quantile(0.20)\n",
    "upper_n_percent_threshold = playlist_sizes.quantile(0.98)\n",
    "\n",
    "# Keep only those playlists within the specified size range\n",
    "valid_playlists = playlist_sizes[\n",
    "    (playlist_sizes >= lower_n_percent_threshold)\n",
    "    & (playlist_sizes <= upper_n_percent_threshold)\n",
    "].index\n",
    "\n",
    "# Filter the DataFrame to keep only valid playlists\n",
    "filtered_df = clean_df[clean_df[\"playlist_and_user_id\"].isin(valid_playlists)]\n",
    "\n",
    "clean_df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest playlist length: 10\n"
     ]
    }
   ],
   "source": [
    "# Recompute the group sizes in the filtered DataFrame\n",
    "final_playlist_sizes = clean_df.groupby(\"playlist_and_user_id\").size()\n",
    "\n",
    "# Find the smallest playlist length\n",
    "smallest_playlist_length = final_playlist_sizes.min()\n",
    "\n",
    "print(\"Smallest playlist length:\", smallest_playlist_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"../data/csv/playlists_dataset/playlist_data_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/csv/playlists_dataset/playlist_data_v3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_playlist_np = clean_df.groupby(\"playlist_and_user_id\")[\n",
    "    \"track_and_artist\"\n",
    "].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_playlist = tokenized_playlist_np.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "temp_arr = tokenized_playlist_np\n",
    "\n",
    "extrapolated_data = tokenized_playlist\n",
    "\n",
    "# Shuffle each sub-array n times and collect results\n",
    "for subarray in temp_arr:\n",
    "    for _ in range(n):\n",
    "        extrapolated_data.append(np.random.permutation(subarray).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735012"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extrapolated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = extrapolated_data\n",
    "\n",
    "# split value\n",
    "split_fac = 0.9\n",
    "\n",
    "max_idx = len(tokenized_data) - 1\n",
    "anchor = int(max_idx * split_fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tokenized_playlist[:anchor]\n",
    "test_set = tokenized_playlist[anchor:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tokenized_data/playlist_names/dataset_train_v3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(training_set, f)\n",
    "\n",
    "\n",
    "with open(\"../data/tokenized_data/playlist_names/dataset_test_v3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing - V2 \n",
    "This is an other part of the notebook to work with the freshly pulled spotify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import copy\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/csv/playlists_dataset/spotify_playlists_fresh_2025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Basic DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15702537 entries, 0 to 15702536\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Dtype  \n",
      "---  ------               -----  \n",
      " 0   playlist_id          object \n",
      " 1   track_id             object \n",
      " 2   track_name           object \n",
      " 3   track_external_urls  object \n",
      " 4   release_date         float64\n",
      " 5   artist_name          object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 718.8+ MB\n",
      "\\n\n",
      "--- Descriptive Statistics for Numerical Columns ---\n",
      "--- Descriptive Statistics for Object/Categorical Columns ---\n",
      "                   playlist_id                track_id track_name  \\\n",
      "count                 15702537                15669982   15673325   \n",
      "unique                  139877                 2862960    1693773   \n",
      "top     2Al9G2jrWkwDlRFMZaw1GX  2WfaOiMkCvy7F5fcp2zZ8L       Home   \n",
      "freq                      3400                    3029       7373   \n",
      "\n",
      "                                      track_external_urls   artist_name  \n",
      "count                                            15669982      15648892  \n",
      "unique                                            2862960        774930  \n",
      "top     https://open.spotify.com/track/2WfaOiMkCvy7F5f...  Taylor Swift  \n",
      "freq                                                 3029         48599  \n",
      "\\n\n",
      "--- Value Counts for Key Columns ---\n",
      "Number of unique playlists:\n",
      "139877\n",
      "Number of unique tracks (by ID):\n",
      "2862960\n",
      "\\n\n",
      "Number of unique track names:\n",
      "1693773\n",
      "\\n\n",
      "--- Artist Analysis ---\n",
      "Number of unique individual artists:\n",
      "542120\n",
      "\\nTop 10 most frequent individual artists:\n",
      "artist_name\n",
      "Bad Bunny         101154\n",
      "Drake              82938\n",
      "Taylor Swift       55653\n",
      "Travis Scott       41744\n",
      "The Weeknd         41665\n",
      "Miracle Tones      41369\n",
      "Kanye West         40851\n",
      "David Guetta       40403\n",
      "Kendrick Lamar     38748\n",
      "Daddy Yankee       37058\n",
      "Name: count, dtype: int64\n",
      "\\n\n",
      "--- Tracks per Playlist Analysis ---\n",
      "playlist_id\n",
      "2Al9G2jrWkwDlRFMZaw1GX    3400\n",
      "1HVP7E8RkpieiGSxvATJbU    3100\n",
      "0vvXsWCC9xrXsKd4FyS8kM    3000\n",
      "6VvcT6f0xh7ODqkwgbmqLF    3000\n",
      "16QguuMuZbadn8Ll3exMpS    2900\n",
      "                          ... \n",
      "6p5wGS3wOFxtf14pYZ6DWt       0\n",
      "0SJjv09HErWsZeGLn3RKXF       0\n",
      "0MsfyJiosw07HzBAEQhTe1       0\n",
      "4RZdbJh7v5Xhuks7rHmFIx       0\n",
      "6JM0cz3FEN2IG7HWXhc9QQ       0\n",
      "Name: track_id, Length: 139877, dtype: int64\n",
      "\\n\n",
      "--- Most Frequent Track Names ---\n",
      "track_name\n",
      "Home                                 7373\n",
      "Winter Ahead (with PARK HYO SHIN)    5911\n",
      "Memories                             5257\n",
      "Paradise                             5117\n",
      "Forever                              5025\n",
      "Stay                                 4949\n",
      "Closer                               4791\n",
      "Heaven                               4457\n",
      "Runaway                              4415\n",
      "Alone                                4046\n",
      "Name: count, dtype: int64\n",
      "\\n\n",
      "--- Missing Values per Column ---\n",
      "playlist_id                   0\n",
      "track_id                  32555\n",
      "track_name                29212\n",
      "track_external_urls       32555\n",
      "release_date           15702537\n",
      "artist_name                   0\n",
      "dtype: int64\n",
      "\\n\n",
      "Basic analysis complete. You can expand on this with more specific questions about your data!\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Basic DataFrame Info ---\")\n",
    "df.info()\n",
    "print(\"\\\\n\")\n",
    "\n",
    "print(\"--- Descriptive Statistics for Numerical Columns ---\")\n",
    "print(\"--- Descriptive Statistics for Object/Categorical Columns ---\")\n",
    "print(df.describe(include=[\"object\"]))\n",
    "print(\"\\\\n\")\n",
    "\n",
    "# --- Value Counts for Key Columns ---\n",
    "print(\"--- Value Counts for Key Columns ---\")\n",
    "print(\"Number of unique playlists:\")\n",
    "print(df[\"playlist_id\"].nunique())\n",
    "\n",
    "print(\"Number of unique tracks (by ID):\")\n",
    "print(df[\"track_id\"].nunique())\n",
    "print(\"\\\\n\")\n",
    "\n",
    "print(\"Number of unique track names:\")\n",
    "print(df[\"track_name\"].nunique())\n",
    "print(\"\\\\n\")\n",
    "\n",
    "# Artist Analysis - Handling comma-separated artists\n",
    "print(\"--- Artist Analysis ---\")\n",
    "# Ensure artist_name is treated as string and handle potential NaNs before splitting\n",
    "df[\"artist_name\"] = df[\"artist_name\"].astype(\n",
    "    str\n",
    ")  # Convert to string to use .str methods\n",
    "all_artists = df[\"artist_name\"].str.split(\",\\\\s*\").explode().str.strip()\n",
    "# Remove 'nan' strings if they resulted from original NaNs converted to string\n",
    "all_artists = all_artists[all_artists.str.lower() != \"nan\"]\n",
    "\n",
    "print(\"Number of unique individual artists:\")\n",
    "print(all_artists.nunique())\n",
    "print(\"\\\\nTop 10 most frequent individual artists:\")\n",
    "print(all_artists.value_counts().nlargest(10))\n",
    "print(\"\\\\n\")\n",
    "\n",
    "# --- Playlist Analysis: Tracks per Playlist ---\n",
    "print(\"--- Tracks per Playlist Analysis ---\")\n",
    "tracks_per_playlist = (\n",
    "    df.groupby(\"playlist_id\")[\"track_id\"].count().sort_values(ascending=False)\n",
    ")\n",
    "print(tracks_per_playlist)\n",
    "print(\"\\\\n\")\n",
    "\n",
    "# --- Most Frequent Track Names ---\n",
    "print(\"--- Most Frequent Track Names ---\")\n",
    "# Consider that track names might not be unique (e.g. remixes, covers, same name different artist)\n",
    "print(df[\"track_name\"].value_counts().nlargest(10))\n",
    "print(\"\\\\n\")\n",
    "\n",
    "# --- Missing Values ---\n",
    "print(\"--- Missing Values per Column ---\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\\\n\")\n",
    "\n",
    "print(\n",
    "    \"Basic analysis complete. You can expand on this with more specific questions about your data!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_external_urls</th>\n",
       "      <th>release_date</th>\n",
       "      <th>artist_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5owFYKDZnehRxuuOLk36iS</td>\n",
       "      <td>1MDyUzZgyrdeQVmV1FU3WQ</td>\n",
       "      <td>Als du gingst - edit</td>\n",
       "      <td>https://open.spotify.com/track/1MDyUzZgyrdeQVm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contec, Lina Maly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5owFYKDZnehRxuuOLk36iS</td>\n",
       "      <td>3t854jxXLppSGbOEYGQ3mI</td>\n",
       "      <td>Shake That</td>\n",
       "      <td>https://open.spotify.com/track/3t854jxXLppSGbO...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sonny Wern, Danimal, okafuwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5owFYKDZnehRxuuOLk36iS</td>\n",
       "      <td>5yujUAF2VoPWKfYMKoBqSK</td>\n",
       "      <td>Wie? - Techno</td>\n",
       "      <td>https://open.spotify.com/track/5yujUAF2VoPWKfY...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FUTURAMI, XEKNO!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5owFYKDZnehRxuuOLk36iS</td>\n",
       "      <td>4zRKaWqz94saRIpW5hPcHC</td>\n",
       "      <td>STARGAZING - TECHNO</td>\n",
       "      <td>https://open.spotify.com/track/4zRKaWqz94saRIp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XEKNO!, TEKTOSHI, VXLTAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5owFYKDZnehRxuuOLk36iS</td>\n",
       "      <td>0bb1R14dsWjFO01iJ6GqF3</td>\n",
       "      <td>SHE DOESN'T MIND - TECHNO</td>\n",
       "      <td>https://open.spotify.com/track/0bb1R14dsWjFO01...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PSYKADELIK, TEKTOSHI, Phantom X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              playlist_id                track_id                 track_name  \\\n",
       "0  5owFYKDZnehRxuuOLk36iS  1MDyUzZgyrdeQVmV1FU3WQ       Als du gingst - edit   \n",
       "1  5owFYKDZnehRxuuOLk36iS  3t854jxXLppSGbOEYGQ3mI                 Shake That   \n",
       "2  5owFYKDZnehRxuuOLk36iS  5yujUAF2VoPWKfYMKoBqSK              Wie? - Techno   \n",
       "3  5owFYKDZnehRxuuOLk36iS  4zRKaWqz94saRIpW5hPcHC        STARGAZING - TECHNO   \n",
       "4  5owFYKDZnehRxuuOLk36iS  0bb1R14dsWjFO01iJ6GqF3  SHE DOESN'T MIND - TECHNO   \n",
       "\n",
       "                                 track_external_urls  release_date  \\\n",
       "0  https://open.spotify.com/track/1MDyUzZgyrdeQVm...           NaN   \n",
       "1  https://open.spotify.com/track/3t854jxXLppSGbO...           NaN   \n",
       "2  https://open.spotify.com/track/5yujUAF2VoPWKfY...           NaN   \n",
       "3  https://open.spotify.com/track/4zRKaWqz94saRIp...           NaN   \n",
       "4  https://open.spotify.com/track/0bb1R14dsWjFO01...           NaN   \n",
       "\n",
       "                       artist_name  \n",
       "0                Contec, Lina Maly  \n",
       "1     Sonny Wern, Danimal, okafuwa  \n",
       "2                 FUTURAMI, XEKNO!  \n",
       "3        XEKNO!, TEKTOSHI, VXLTAGE  \n",
       "4  PSYKADELIK, TEKTOSHI, Phantom X  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15702537\n",
      "10440545\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df_cleaned = df.drop_duplicates(subset=[\"playlist_id\", \"track_id\"], keep=\"first\")\n",
    "print(len(df_cleaned))\n",
    "\n",
    "canonical_info_map = (\n",
    "    df_cleaned.groupby([\"artist_name\", \"track_name\"])\n",
    "    .first()[[\"track_id\", \"track_external_urls\"]]\n",
    "    .to_dict(orient=\"index\")\n",
    ")\n",
    "\n",
    "\n",
    "def map_to_canonical(row):\n",
    "    key = (row[\"artist_name\"], row[\"track_name\"])\n",
    "    canonical = canonical_info_map.get(key, {})\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"canonical_track_id\": canonical.get(\"track_id\", row[\"track_id\"]),\n",
    "            \"canonical_track_external_urls\": canonical.get(\n",
    "                \"track_external_urls\", row[\"track_external_urls\"]\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "df_cleaned[[\"canonical_track_id\", \"canonical_track_external_urls\"]] = df_cleaned.apply(\n",
    "    map_to_canonical, axis=1\n",
    ")\n",
    "\n",
    "df_remapped = df_cleaned.drop_duplicates(\n",
    "    subset=[\"playlist_id\", \"canonical_track_id\"], keep=\"first\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = df_cleaned.groupby(\"playlist_id\")[\"track_id\"].apply(list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139877"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = len(tokenized_data)\n",
    "\n",
    "split_90_percent = int(data_length * 0.90)\n",
    "split_95_percent = int(data_length * 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_and_shuffle(data_set, multiplication_factor, seed):\n",
    "    \"\"\"\n",
    "    Vervielfacht jede innere Liste in einem Datensatz und mischt jede Kopie intern.\n",
    "\n",
    "    Args:\n",
    "        data_set (list): Der ursprüngliche Datensatz (eine Liste von Listen).\n",
    "        multiplication_factor (int): Wie oft jede innere Liste vervielfacht werden soll.\n",
    "        seed (int): Der Seed für die Zufallszahlengenerierung, um Reproduzierbarkeit zu gewährleisten.\n",
    "\n",
    "    Returns:\n",
    "        list: Der extrapolierte und gemischte Datensatz.\n",
    "    \"\"\"\n",
    "    extrapolated_data = []\n",
    "    # Verwenden Sie eine Instanz von random.Random für isolierte Seed-Verwendung\n",
    "    # Dies verhindert, dass der Seed andere Zufallsoperationen beeinflusst\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    for inner_list in data_set:\n",
    "        for i in range(multiplication_factor):\n",
    "            # Erstellen Sie eine tiefe Kopie der inneren Liste, um unabhängige Mischungen zu ermöglichen\n",
    "            shuffled_copy = copy.deepcopy(inner_list)\n",
    "            rng.shuffle(shuffled_copy)\n",
    "            extrapolated_data.append(shuffled_copy)\n",
    "    return extrapolated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tokenized_data[:split_90_percent]\n",
    "validation_set = tokenized_data[split_90_percent:split_95_percent]\n",
    "test_set = tokenized_data[split_95_percent:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolate data\n",
    "multiplication_factor = 3\n",
    "my_seed = 42\n",
    "\n",
    "extrapolated_training_set = extrapolate_and_shuffle(\n",
    "    training_set, multiplication_factor, my_seed\n",
    ")\n",
    "extrapolated_validation_set = extrapolate_and_shuffle(\n",
    "    validation_set, multiplication_factor, my_seed + 1\n",
    ")\n",
    "extrapolated_test_set = extrapolate_and_shuffle(\n",
    "    test_set, multiplication_factor, my_seed + 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377667\n",
      "20982\n",
      "20982\n"
     ]
    }
   ],
   "source": [
    "# size of the extrapolated data\n",
    "print(len(extrapolated_training_set))\n",
    "print(len(extrapolated_validation_set))\n",
    "print(len(extrapolated_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"../data/tokenized_data/playlist_names/fresh_dataset_train_v2.pkl\", \"wb\"\n",
    ") as f:\n",
    "    pickle.dump(extrapolated_training_set, f)\n",
    "\n",
    "with open(\"../data/tokenized_data/playlist_names/fresh_dataset_val_v2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extrapolated_validation_set, f)\n",
    "\n",
    "with open(\"../data/tokenized_data/playlist_names/fresh_dataset_test_v2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extrapolated_test_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/csv/playlists_dataset/spotify_playlists_fresh_2025.csv\")\n",
    "\n",
    "# playlist_id,track_id,track_name,track_external_urls,release_date,artist_name\n",
    "df_cleaned = df.drop_duplicates(subset=[\"track_id\"], keep=\"first\")\n",
    "# trackname,artistname,track_and_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"../data/csv/songs/songs2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
